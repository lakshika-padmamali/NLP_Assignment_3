general Attention:
Train Loss: [5.195961243127857, 4.1540533114279, 3.6557893238000037, 3.329062706348242, 3.095558759674513, 2.9245824442095665, 2.794370659628924, 2.6933232184991502, 2.6099210054523145, 2.5412990299024423]
Validation Loss: [4.463782812563306, 3.9695991320330357, 3.708117845687175, 3.5489828585441416, 3.451971231668861, 3.3831622402725983, 3.3409666884326343, 3.307743368458054, 3.2770787845296865, 3.258658463294357]

multiplicative Attention:
Train Loss: [5.211436667721132, 4.176525629068795, 3.679360430036235, 3.348310575134929, 3.1131379879575443, 2.938998131737716, 2.806710124438248, 2.7049551228663247, 2.6210778448129033, 2.5516432388221695]
Validation Loss: [4.484221180677983, 3.990862662188485, 3.727132548590075, 3.5590212942022452, 3.4714212483545683, 3.3912956716335554, 3.349671381794389, 3.3033188065859496, 3.2837865773302406, 3.253677353270007]

additive Attention:
Train Loss: [5.207653856673909, 4.159037772228433, 3.659144797376477, 3.3302373170560204, 3.0986352890051565, 2.926376883675231, 2.7971257231809696, 2.6936127601859843, 2.6134651520279197, 2.5452310306687633]
Validation Loss: [4.484059184064852, 3.971217542360894, 3.7072216195609946, 3.5539507871590517, 3.4586839555386537, 3.386116039087844, 3.3464526087543085, 3.3066757475698796, 3.284933805408851, 3.259302804057349]
